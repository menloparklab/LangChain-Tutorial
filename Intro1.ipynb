{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8f120",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY=\n",
    "%env COHERE_API_KEY="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d83b52",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why LangChain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8d82c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### There are many reasons, let's look at one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2202ee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's say we want OpenAI GPT models to write us a joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c25c1510",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"Tell me a joke on procrastination\",\n",
    "  max_tokens=35,\n",
    "  temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4260174a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7TFFyHU8Q2UTm8uasE2vieHSQfhfQ at 0x1130d7170> JSON: {\n",
       "  \"id\": \"cmpl-7TFFyHU8Q2UTm8uasE2vieHSQfhfQ\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1687204514,\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \"\\n\\nQ: Why did the procrastinator eat breakfast in bed?\\nA: Because he wanted to procrastinate on getting out of it.\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 8,\n",
       "    \"completion_tokens\": 31,\n",
       "    \"total_tokens\": 39\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a89f71ee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nQ: Why did the procrastinator eat breakfast in bed?\\nA: Because he wanted to procrastinate on getting out of it.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e80750",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import cohere\n",
    "co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "response_cohere = co.generate(\n",
    "  prompt='Tell me a joke on procrastination',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a4a6fce",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tell me a joke on procrastination</td>\n",
       "      <td>Why did the student put off writing his essay?\n",
       "Because he was too lazy to start!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Generations([cohere.Generation {\n",
       "             \tid: 90745e3e-d287-48b5-946e-2603e2fca6d3\n",
       "             \tprompt: Tell me a joke on procrastination\n",
       "             \ttext: \n",
       "             Why did the student put off writing his essay?\n",
       "             Because he was too lazy to start!\n",
       "             \tlikelihood: None\n",
       "             \tfinish_reason: None\n",
       "             \ttoken_likelihoods: None\n",
       "             }])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b924569",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why did the student put off writing his essay?\n",
      "Because he was too lazy to start!\n"
     ]
    }
   ],
   "source": [
    "print(response_cohere[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99387234",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How about if you would like to call LLMs from other providers/platforms\n",
    "\n",
    "- AI21\n",
    "- Aleph Alpha\n",
    "- Hugging Face Hub\n",
    "- GPT4All\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae7e37",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's try again with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4943341",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51f08d30",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nQ: Why did the procrastinator cross the road?\\nA: He'll do it tomorrow!\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"Tell me a joke on procrastination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbadeab5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Cohere\n",
    "\n",
    "llm = Cohere()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b523e952",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhy did the student put off studying for his exam?\\nBecause he was too lazy to start.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"Tell me a joke on procrastination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7f6f55",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Most of the heavylifting is done for you, just a swap of provider/platform, and you are all set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e75e63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### What if you would like to swap the joke topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e815e449",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"Tell me a joke on {user_topic}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e28f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "joke_template =\"Tell me a joke on {user_topic}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d5b2ea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use the Prompt Templates available in LangChain and supply the joke template created\n",
    "\n",
    "```\n",
    "joke_template =\"Tell me a joke on {user_topic}\"  \n",
    "\n",
    "PromptTemplate(template=joke_template)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296388a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Also specify the input variables, in our case - user_topic\n",
    "\n",
    "```\n",
    "PromptTemplate(template=joke_template, \n",
    "                input_variables=[\"user_topic\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52d3306d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "joke_template =\"Tell me a joke on {user_topic}\"\n",
    "\n",
    "prompt = PromptTemplate(template=joke_template, input_variables=[\"user_topic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "862db15e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a joke on procrastination'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(user_topic=\"procrastination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "151bcf44",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "final_prompt = prompt.format(user_topic=\"procrastination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9980418c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhy did the student put off his homework?\\nHe wanted to make it a perfect ten.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d37f13",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### What if we want to get the user_topic for the joke, and also have the flexibility to use different LLM providers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f069c32",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example:  \n",
    "\n",
    "prompt = PromptTemplate  \n",
    "user_topic = \"taxes\"  \n",
    "\n",
    "llm = \"Cohere\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb8b83",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Old:  \n",
    "```\n",
    "prompt = PromptTemplate(template=joke_template, \n",
    "                        input_variables=[\"user_topic\"])  \n",
    "                        \n",
    "final_prompt = prompt.format(user_topic=\"taxes\")  \n",
    "\n",
    "\n",
    "llm.predict(final_prompt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3233d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### For that we chain LLM and prompt together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5da352",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "New:  \n",
    "```\n",
    "prompt = PromptTemplate(template=joke_template, \n",
    "                        input_variables=[\"user_topic\"])  \n",
    "llm= Cohere()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8868f31",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "LLMChain(prompt=prompt, llm=Cohere())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "890349a4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm_joke_chain = LLMChain(prompt=prompt, llm=Cohere())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfafe504",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhy did the taxpayer cross the road?\\nTo get to the other tax office.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_joke_chain.predict(user_topic=\"taxes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d4b3d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How about a list of user_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7831ef9f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '\\nWhy did the student put off studying for the exam?\\nBecause he knew the teach would give him extra time to study.'},\n",
       " {'text': '\\nWhy did the taxpayer cross the road?\\nBecause the other side had a lower tax rate.'},\n",
       " {'text': '\\nWhy did the Canadian cross the road?\\nTo get to the Tim Hortons on the other side!'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list = [\n",
    "    {\"user_topic\": \"procrastination\"},\n",
    "    {\"user_topic\": \"taxes\"},\n",
    "    {\"user_topic\": \"Canada\"}\n",
    "]\n",
    "\n",
    "llm_joke_chain.apply(input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ba124",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### What if you would like to first get a joke, and then write a NY Times Article about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873cde9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Use Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48cb8b9e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=.7)\n",
    "template = \"\"\"You are a NY Times Editor. Given the joke by an AI, it is your job to write a NY Times article on the joke.\n",
    "\n",
    "Joke: {joke}\n",
    "Article: This is the article about the joke above:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"joke\"], template=template)\n",
    "ny_times_article_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58af3a41",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# This is the overall chain where we run these two chains in sequence.\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[llm_joke_chain, ny_times_article_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25a6bf3f",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "Why did the Canadian cross the road?\n",
      "To get to the Tim Hortons on the other side!\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "Canadians are known for their love of timbits, poutine, and maple syrup, but the latest trend for Canadians is crossing the road.\n",
      "\n",
      "That's right, Canadians are crossing the road to get to the one place they love most, Tim Hortons.\n",
      "\n",
      "The \"Tim Hortons Crossing\" phenomenon has been sweeping the nation. People are leaving their homes and braving traffic to get to the beloved coffee chain.\n",
      "\n",
      "So why are Canadians taking this risk? It's all for a good cup of coffee and a donut or two. Many people say they just can't resist the warm atmosphere and friendly staff at Tim Hortons.\n",
      "\n",
      "It's clear that Tim Hortons has become a national institution for Canadians. It's not just a place to get coffee, but a place to meet up with friends or just relax and enjoy the moment.\n",
      "\n",
      "So, the next time you see a Canadian crossing the road, you'll know why.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "article = overall_chain.run(\"Canada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca64ffc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How about we ask a question from the provided story?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c2ecbb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can use a Question Answering chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b77f2d85",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "qa_chain = load_qa_chain(llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b47b5a9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import AnalyzeDocumentChain\n",
    "qa_document_chain = AnalyzeDocumentChain(combine_docs_chain=qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f795f5ac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Canadians are known for their love of timbits, poutine, and maple syrup, and they have recently been crossing roads to get to Tim Hortons for a good cup of coffee and donut or two. They find the atmosphere and staff at Tim Hortons welcoming, and it has become a national institution.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_document_chain.run( input_document=article, \n",
    "                          question=\"what is mentioned about Canadians\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4fe672",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### What if we want to ask question from a large text body eg. books, policies, code bases etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f4903",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We use a path similar to the diagram below:\n",
    "![diagram](https://langchain-langchain.vercel.app/assets/images/data_connection-c42d68c3d092b85f50d08d4cc171fc25.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d24ff1dc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import CohereEmbeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "154391d7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "loader = TextLoader('state_of_the_union.txt', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64741c0d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(embedding=CohereEmbeddings()).from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "024a0541",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The president said that Ketanji Brown Jackson is one of the nation's top legal minds and that she will continue Justice Breyer's legacy of excellence.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "index.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beca151",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### There is a tremendous opportunity to combine the hundreds of loaders, embedding models, vectorstores, and retrievers. Build something fun and share with the community!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc81655f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What if we want our application to remember our conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c266cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### We can add \"memory\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c556572f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58087efb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "memory_chain = ConversationChain(llm=llm, memory=ConversationBufferMemory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fa68456",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hi Misbah! My name is AI-1. It's nice to meet you.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_chain.predict(input=\"Hi there! My name is Misbah, what's your name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7111a3f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Sure, your name is Misbah.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_chain.predict(input=\"Can you remind me what's my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae82280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
